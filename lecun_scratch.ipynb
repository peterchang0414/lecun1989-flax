{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lecun_scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peterchang0414/lecun1989-flax/blob/main/lecun_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1989 Reproduction"
      ],
      "metadata": {
        "id": "N7JVDrbphkxe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oe2o-uZZd4yT"
      },
      "outputs": [],
      "source": [
        "!pip install -q flax"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "from flax import linen as nn\n",
        "from torchvision import datasets\n",
        "\n",
        "def get_datasets(n_tr, n_te):\n",
        "  train_test = {}\n",
        "  for split in {'train', 'test'}:\n",
        "    data = datasets.MNIST('./data', train=split=='train', download=True)\n",
        "    \n",
        "    n = n_tr if split == 'train' else n_te\n",
        "    key = jax.random.PRNGKey(42)\n",
        "    rp = jax.random.permutation(key, len(data))[:n]\n",
        "\n",
        "    X = jnp.full((n, 16, 16, 1), 0.0, dtype=jnp.float32)\n",
        "    Y = jnp.full((n, 10), -1.0, dtype=jnp.float32)\n",
        "    for i, ix in enumerate(rp):\n",
        "      I, yint = data[int(ix)]\n",
        "      xi = jnp.array(I, dtype=jnp.float32) / 127.5 - 1.0\n",
        "      xi = jax.image.resize(xi, (16, 16), 'bilinear')\n",
        "      X = X.at[i].set(jnp.expand_dims(xi, axis=2))\n",
        "      Y = Y.at[i, yint].set(1.0)\n",
        "    train_test[split] = (X, Y)\n",
        "  return train_test"
      ],
      "metadata": {
        "id": "pc7McMNRnt2E"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flax import linen as nn\n",
        "from flax.training import train_state\n",
        "from flax.linen.activation import tanh\n",
        "import optax\n",
        "from typing import Callable\n",
        "\n",
        "class Net(nn.Module):\n",
        "  bias_init: Callable = nn.initializers.zeros\n",
        "  # sqrt(6) = 2.449... used by he_uniform() approximates Karpathy's 2.4\n",
        "  kernel_init: Callable = nn.initializers.he_uniform()\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    x = jnp.pad(x, [(0,0),(2,2),(2,2),(0,0)], constant_values=-1.0)\n",
        "    x = nn.Conv(features=12, kernel_size=(5,5), strides=2, padding='VALID',\n",
        "                use_bias=False, kernel_init=self.kernel_init)(x)\n",
        "    bias1 = self.param('bias1', self.bias_init, (8, 8, 12))\n",
        "    x = tanh(x + bias1)\n",
        "    x = jnp.pad(x, [(0,0),(2,2),(2,2),(0,0)], constant_values=-1.0)\n",
        "    x1, x2, x3 = (x[..., 0:8], x[..., 4:12], \n",
        "                  jnp.concatenate((x[..., 0:4], x[..., 8:12]), axis=-1))\n",
        "    slice1 = nn.Conv(features=4, kernel_size=(5,5), strides=2, padding='VALID', \n",
        "                     use_bias=False, kernel_init=self.kernel_init)(x1)\n",
        "    slice2 = nn.Conv(features=4, kernel_size=(5,5), strides=2, padding='VALID',\n",
        "                     use_bias=False, kernel_init=self.kernel_init)(x2)\n",
        "    slice3 = nn.Conv(features=4, kernel_size=(5,5), strides=2, padding='VALID',\n",
        "                     use_bias=False, kernel_init=self.kernel_init)(x3)\n",
        "    x = jnp.concatenate((slice1, slice2, slice3), axis=-1)\n",
        "    bias2 = self.param('bias2', self.bias_init, (4, 4, 12))\n",
        "    x = tanh(x + bias2)\n",
        "    x = x.reshape((x.shape[0], -1))\n",
        "    x = nn.Dense(features=30, use_bias=False)(x)\n",
        "    bias3 = self.param('bias3', self.bias_init, (30,))\n",
        "    x = tanh(x + bias3)\n",
        "    x = nn.Dense(features=10, use_bias=False)(x)\n",
        "    bias4 = self.param('bias4', nn.initializers.constant(-1.0), (10,))\n",
        "    x = tanh(x + bias4)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "hk3obOlvqoDZ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def eval_step(params, X, Y):\n",
        "  Yhat = Net().apply({'params': params}, X)\n",
        "  loss = jnp.mean((Yhat - Y)**2)\n",
        "  err = jnp.mean(jnp.argmax(Y, -1) != jnp.argmax(Yhat, -1)).astype(float)\n",
        "  return loss, err"
      ],
      "metadata": {
        "id": "KUCi-jvDXuha"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_split(data, split, params):\n",
        "  X, Y = data[split]\n",
        "  loss, err = eval_step(params, X, Y)\n",
        "  print(f\"eval: split {split:5s}. loss {loss:e}. error {err*100:.2f}%. misses: {int(err*Y.shape[0])}\")"
      ],
      "metadata": {
        "id": "D9KDZtpgVUBu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import value_and_grad\n",
        "import optax\n",
        "from flax.training import train_state\n",
        "\n",
        "def create_train_state(key, lr, X):\n",
        "  model = Net()\n",
        "  params = model.init(key, X)['params']\n",
        "  sgd_opt = optax.sgd(lr)\n",
        "  return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=sgd_opt)\n",
        "\n",
        "@jax.jit\n",
        "def train_step(state, X, Y):\n",
        "  def loss_fn(params):\n",
        "    Yhat = Net().apply({'params': params}, X)\n",
        "    loss = jnp.mean((Yhat - Y)**2)\n",
        "    err = jnp.mean(jnp.argmax(Y, -1) != jnp.argmax(Yhat, -1)).astype(float)\n",
        "    return loss, err\n",
        "  (_, Yhats), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
        "  state = state.apply_gradients(grads=grads)\n",
        "  return state\n",
        "\n",
        "def train_one_epoch(state, X, Y):\n",
        "  for step_num in range(X.shape[0]):\n",
        "    x, y = jnp.expand_dims(X[step_num], 0), jnp.expand_dims(Y[step_num], 0)\n",
        "    state = train_step(state, x, y)\n",
        "  return state\n",
        "\n",
        "def train(key, data, epochs, lr):\n",
        "  Xtr, Ytr = data['train']\n",
        "  Xte, Yte = data['test']\n",
        "  train_state = create_train_state(key, lr, Xtr)\n",
        "  for epoch in range(epochs):\n",
        "    print(f\"epoch {epoch+1}\")\n",
        "    train_state = train_one_epoch(train_state, Xtr, Ytr)\n",
        "    for split in ['train', 'test']:\n",
        "      eval_split(data, split, train_state.params)\n"
      ],
      "metadata": {
        "id": "nwzevZv3CdvA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = get_datasets(7291, 2007)"
      ],
      "metadata": {
        "id": "phiwVJtuFerc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key, _ = jax.random.split(jax.random.PRNGKey(42))\n",
        "\n",
        "train(key, data, 23, 0.03)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZhSr9-YKlo5",
        "outputId": "56fac0fa-e157-4eab-a5ec-b98846758252"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1\n",
            "eval: split train. loss 5.576071e-02. error 8.11%. misses: 591\n",
            "eval: split test . loss 5.287848e-02. error 7.37%. misses: 148\n",
            "epoch 2\n",
            "eval: split train. loss 4.097378e-02. error 5.80%. misses: 423\n",
            "eval: split test . loss 4.257497e-02. error 6.08%. misses: 122\n",
            "epoch 3\n",
            "eval: split train. loss 3.390130e-02. error 4.92%. misses: 359\n",
            "eval: split test . loss 3.796291e-02. error 5.48%. misses: 110\n",
            "epoch 4\n",
            "eval: split train. loss 2.989994e-02. error 4.38%. misses: 319\n",
            "eval: split test . loss 3.480190e-02. error 5.23%. misses: 105\n",
            "epoch 5\n",
            "eval: split train. loss 2.566473e-02. error 3.77%. misses: 275\n",
            "eval: split test . loss 3.232093e-02. error 4.73%. misses: 95\n",
            "epoch 6\n",
            "eval: split train. loss 2.348944e-02. error 3.33%. misses: 242\n",
            "eval: split test . loss 3.208887e-02. error 4.58%. misses: 92\n",
            "epoch 7\n",
            "eval: split train. loss 2.151174e-02. error 3.09%. misses: 225\n",
            "eval: split test . loss 3.206819e-02. error 4.93%. misses: 99\n",
            "epoch 8\n",
            "eval: split train. loss 1.941714e-02. error 2.77%. misses: 202\n",
            "eval: split test . loss 3.061979e-02. error 4.73%. misses: 95\n",
            "epoch 9\n",
            "eval: split train. loss 1.694829e-02. error 2.41%. misses: 176\n",
            "eval: split test . loss 2.916610e-02. error 4.38%. misses: 88\n",
            "epoch 10\n",
            "eval: split train. loss 1.605429e-02. error 2.22%. misses: 162\n",
            "eval: split test . loss 2.967581e-02. error 4.58%. misses: 92\n",
            "epoch 11\n",
            "eval: split train. loss 1.565071e-02. error 2.18%. misses: 159\n",
            "eval: split test . loss 3.011220e-02. error 4.58%. misses: 92\n",
            "epoch 12\n",
            "eval: split train. loss 1.397184e-02. error 1.93%. misses: 141\n",
            "eval: split test . loss 2.919692e-02. error 4.53%. misses: 91\n",
            "epoch 13\n",
            "eval: split train. loss 1.240323e-02. error 1.59%. misses: 116\n",
            "eval: split test . loss 2.727516e-02. error 3.64%. misses: 73\n",
            "epoch 14\n",
            "eval: split train. loss 1.198561e-02. error 1.56%. misses: 114\n",
            "eval: split test . loss 2.697299e-02. error 3.89%. misses: 78\n",
            "epoch 15\n",
            "eval: split train. loss 1.133908e-02. error 1.44%. misses: 105\n",
            "eval: split test . loss 2.733141e-02. error 3.94%. misses: 79\n",
            "epoch 16\n",
            "eval: split train. loss 1.065093e-02. error 1.47%. misses: 107\n",
            "eval: split test . loss 2.849034e-02. error 4.09%. misses: 82\n",
            "epoch 17\n",
            "eval: split train. loss 9.458693e-03. error 1.26%. misses: 92\n",
            "eval: split test . loss 2.668566e-02. error 3.79%. misses: 76\n",
            "epoch 18\n",
            "eval: split train. loss 7.680640e-03. error 1.08%. misses: 79\n",
            "eval: split test . loss 2.510950e-02. error 3.74%. misses: 75\n",
            "epoch 19\n",
            "eval: split train. loss 6.790097e-03. error 1.00%. misses: 73\n",
            "eval: split test . loss 2.578570e-02. error 3.69%. misses: 74\n",
            "epoch 20\n",
            "eval: split train. loss 6.345607e-03. error 0.93%. misses: 68\n",
            "eval: split test . loss 2.508449e-02. error 3.54%. misses: 71\n",
            "epoch 21\n",
            "eval: split train. loss 5.988171e-03. error 0.92%. misses: 66\n",
            "eval: split test . loss 2.509341e-02. error 3.59%. misses: 72\n",
            "epoch 22\n",
            "eval: split train. loss 5.771732e-03. error 0.88%. misses: 64\n",
            "eval: split test . loss 2.479325e-02. error 3.54%. misses: 71\n",
            "epoch 23\n",
            "eval: split train. loss 5.265484e-03. error 0.82%. misses: 60\n",
            "eval: split test . loss 2.467080e-02. error 3.69%. misses: 74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results:\n",
        "\n",
        "```\n",
        "epoch 23\n",
        "eval: split train. loss 5.265484e-03. error 0.82%. misses: 60\n",
        "eval: split test . loss 2.467080e-02. error 3.69%. misses: 74\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "rOaujFXQyqjd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# \"Modern\" Adjustments"
      ],
      "metadata": {
        "id": "cNEUr0vSfh1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q flax"
      ],
      "metadata": {
        "id": "vLDKA_qjhy6S"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "from flax import linen as nn\n",
        "from torchvision import datasets\n",
        "\n",
        "def get_datasets(n_tr, n_te):\n",
        "  train_test = {}\n",
        "  for split in {'train', 'test'}:\n",
        "    data = datasets.MNIST('./data', train=split=='train', download=True)\n",
        "    \n",
        "    n = n_tr if split == 'train' else n_te\n",
        "    key = jax.random.PRNGKey(42)\n",
        "    rp = jax.random.permutation(key, len(data))[:n]\n",
        "\n",
        "    X = jnp.full((n, 16, 16, 1), 0.0, dtype=jnp.float32)\n",
        "    Y = jnp.full((n, 10), 0, dtype=jnp.float32)\n",
        "    for i, ix in enumerate(rp):\n",
        "      I, yint = data[int(ix)]\n",
        "      xi = jnp.array(I, dtype=jnp.float32) / 127.5 - 1.0\n",
        "      xi = jax.image.resize(xi, (16, 16), 'bilinear')\n",
        "      X = X.at[i].set(jnp.expand_dims(xi, axis=2))\n",
        "      Y = Y.at[i, yint].set(1.0)\n",
        "    train_test[split] = (X, Y)\n",
        "  return train_test"
      ],
      "metadata": {
        "id": "m68J20OSh0rh"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flax import linen as nn\n",
        "from flax.training import train_state\n",
        "from flax.linen.activation import tanh\n",
        "import optax\n",
        "from typing import Callable\n",
        "\n",
        "class Net(nn.Module):\n",
        "  training: bool\n",
        "  bias_init: Callable = nn.initializers.zeros\n",
        "  # sqrt(6) = 2.449... used by he_uniform() approximates Karpathy's 2.4\n",
        "  kernel_init: Callable = nn.initializers.he_uniform()\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    if self.training:\n",
        "      augment_rng = self.make_rng('aug')\n",
        "      shift_x, shift_y = jax.random.randint(augment_rng, (2,), -1, 2)\n",
        "      x = jnp.roll(x, (shift_x, shift_y), (1, 2))\n",
        "\n",
        "    x = jnp.pad(x, [(0,0),(2,2),(2,2),(0,0)], constant_values=-1.0)\n",
        "    x = nn.Conv(features=12, kernel_size=(5,5), strides=2, padding='VALID',\n",
        "                use_bias=False, kernel_init=self.kernel_init)(x)\n",
        "    bias1 = self.param('bias1', self.bias_init, (8, 8, 12))\n",
        "    x = nn.relu(x + bias1)\n",
        "    x = jnp.pad(x, [(0,0),(2,2),(2,2),(0,0)], constant_values=-1.0)\n",
        "    x1, x2, x3 = (x[..., 0:8], x[..., 4:12], \n",
        "                  jnp.concatenate((x[..., 0:4], x[..., 8:12]), axis=-1))\n",
        "    slice1 = nn.Conv(features=4, kernel_size=(5,5), strides=2, padding='VALID', \n",
        "                     use_bias=False, kernel_init=self.kernel_init)(x1)\n",
        "    slice2 = nn.Conv(features=4, kernel_size=(5,5), strides=2, padding='VALID',\n",
        "                     use_bias=False, kernel_init=self.kernel_init)(x2)\n",
        "    slice3 = nn.Conv(features=4, kernel_size=(5,5), strides=2, padding='VALID',\n",
        "                     use_bias=False, kernel_init=self.kernel_init)(x3)\n",
        "    x = jnp.concatenate((slice1, slice2, slice3), axis=-1)\n",
        "    bias2 = self.param('bias2', self.bias_init, (4, 4, 12))\n",
        "    x = nn.relu(x + bias2)\n",
        "    x = nn.Dropout(0.25, deterministic=not self.training)(x)\n",
        "\n",
        "    x = x.reshape((x.shape[0], -1))\n",
        "    x = nn.Dense(features=30, use_bias=False)(x)\n",
        "    bias3 = self.param('bias3', self.bias_init, (30,))\n",
        "    x = nn.relu(x + bias3)\n",
        "    x = nn.Dense(features=10, use_bias=False)(x)\n",
        "    bias4 = self.param('bias4', self.bias_init, (10,))\n",
        "    x = x + bias4\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "voQJfHf6thIH"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import value_and_grad\n",
        "import optax\n",
        "from flax.training import train_state\n",
        "\n",
        "def learning_rate_fn(initial_rate, epochs, steps_per_epoch):\n",
        "  return optax.linear_schedule(init_value=initial_rate, end_value=initial_rate/3,\n",
        "                               transition_steps=epochs*steps_per_epoch)\n",
        "\n",
        "def create_train_state(key, X, lr_fn):\n",
        "  model = Net(training=True)\n",
        "  key1, key2, key3 = jax.random.split(key, 3)\n",
        "  params = model.init({'params': key1, 'aug': key2, 'dropout': key3}, X)['params']\n",
        "  opt = optax.adamw(lr_fn)\n",
        "  return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=opt)\n",
        "\n",
        "@jax.jit\n",
        "def train_step(state, X, Y, rng=jax.random.PRNGKey(0)):\n",
        "  aug_rng, dropout_rng = jax.random.split(jax.random.fold_in(rng, state.step))\n",
        "  def loss_fn(params):\n",
        "    Yhat = Net(training=True).apply({'params': params}, X, \n",
        "                                    rngs={'aug': aug_rng,\n",
        "                                          'dropout': dropout_rng})\n",
        "    loss = jnp.mean(optax.softmax_cross_entropy(logits=Yhat, labels=Y))\n",
        "    err = jnp.mean(jnp.argmax(Y, -1) != jnp.argmax(Yhat, -1)).astype(float)\n",
        "    return loss, err\n",
        "  (_, Yhats), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
        "  state = state.apply_gradients(grads=grads)\n",
        "  return state\n",
        "\n",
        "def train_one_epoch(state, X, Y):\n",
        "  for step_num in range(X.shape[0]):\n",
        "    x, y = jnp.expand_dims(X[step_num], 0), jnp.expand_dims(Y[step_num], 0)\n",
        "    state = train_step(state, x, y)\n",
        "  return state\n",
        "\n",
        "def train(key, data, epochs, lr):\n",
        "  Xtr, Ytr = data['train']\n",
        "  Xte, Yte = data['test']\n",
        "  lr_fn = learning_rate_fn(lr, epochs, Xtr.shape[0])\n",
        "  train_state = create_train_state(key, Xtr, lr_fn)\n",
        "  for epoch in range(epochs):\n",
        "    print(f\"epoch {epoch+1} with learning rate {lr_fn(train_state.step):.6f}\")\n",
        "    train_state = train_one_epoch(train_state, Xtr, Ytr)\n",
        "    for split in ['train', 'test']:\n",
        "      eval_split(data, split, train_state.params)\n"
      ],
      "metadata": {
        "id": "emEVm2HViete",
        "outputId": "bd3367ca-34d9-4dfc-d5f3-6108852a0310",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def eval_step(params, X, Y):\n",
        "  Yhat = Net(training=False).apply({'params': params}, X)\n",
        "  loss = jnp.mean(optax.softmax_cross_entropy(logits=Yhat, labels=Y))\n",
        "  err = jnp.mean(jnp.argmax(Y, -1) != jnp.argmax(Yhat, -1)).astype(float)\n",
        "  return loss, err\n",
        "\n",
        "def eval_split(data, split, params):\n",
        "  X, Y = data[split]\n",
        "  loss, err = eval_step(params, X, Y)\n",
        "  print(f\"eval: split {split:5s}. loss {loss:e}. error {err*100:.2f}%. misses: {int(err*Y.shape[0])}\")"
      ],
      "metadata": {
        "id": "fRywm7o5jxPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = get_datasets(7291, 2007)"
      ],
      "metadata": {
        "id": "G5D2wrzjEayd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key, _ = jax.random.split(jax.random.PRNGKey(42))\n",
        "\n",
        "train(key, data, 80, 3e-4)"
      ],
      "metadata": {
        "id": "2vncyC3wohuc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b8a8c28-9dea-49ab-af59-2aa65f9802c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 with learning rate 0.000300\n",
            "eval: split train. loss 5.306527e-01. error 15.50%. misses: 1130\n",
            "eval: split test . loss 4.991272e-01. error 14.10%. misses: 283\n",
            "epoch 2 with learning rate 0.000297\n",
            "eval: split train. loss 3.483875e-01. error 9.53%. misses: 694\n",
            "eval: split test . loss 3.145785e-01. error 9.07%. misses: 182\n",
            "epoch 3 with learning rate 0.000295\n",
            "eval: split train. loss 2.761495e-01. error 8.01%. misses: 584\n",
            "eval: split test . loss 2.458399e-01. error 7.52%. misses: 151\n",
            "epoch 4 with learning rate 0.000292\n",
            "eval: split train. loss 2.563986e-01. error 7.42%. misses: 541\n",
            "eval: split test . loss 2.329497e-01. error 7.62%. misses: 153\n",
            "epoch 5 with learning rate 0.000290\n",
            "eval: split train. loss 2.165796e-01. error 6.31%. misses: 459\n",
            "eval: split test . loss 1.946320e-01. error 6.03%. misses: 121\n",
            "epoch 6 with learning rate 0.000288\n",
            "eval: split train. loss 1.936624e-01. error 5.51%. misses: 402\n",
            "eval: split test . loss 1.731407e-01. error 5.68%. misses: 114\n",
            "epoch 7 with learning rate 0.000285\n",
            "eval: split train. loss 1.861464e-01. error 5.05%. misses: 368\n",
            "eval: split test . loss 1.620272e-01. error 4.93%. misses: 99\n",
            "epoch 8 with learning rate 0.000282\n",
            "eval: split train. loss 1.633792e-01. error 4.75%. misses: 346\n",
            "eval: split test . loss 1.463806e-01. error 4.73%. misses: 95\n",
            "epoch 9 with learning rate 0.000280\n",
            "eval: split train. loss 1.554337e-01. error 4.72%. misses: 344\n",
            "eval: split test . loss 1.362883e-01. error 4.14%. misses: 83\n",
            "epoch 10 with learning rate 0.000277\n",
            "eval: split train. loss 1.465923e-01. error 4.39%. misses: 320\n",
            "eval: split test . loss 1.288402e-01. error 4.33%. misses: 87\n",
            "epoch 11 with learning rate 0.000275\n",
            "eval: split train. loss 1.428753e-01. error 4.06%. misses: 296\n",
            "eval: split test . loss 1.301340e-01. error 3.89%. misses: 78\n",
            "epoch 12 with learning rate 0.000273\n",
            "eval: split train. loss 1.295899e-01. error 3.94%. misses: 287\n",
            "eval: split test . loss 1.182394e-01. error 3.84%. misses: 77\n",
            "epoch 13 with learning rate 0.000270\n",
            "eval: split train. loss 1.257182e-01. error 3.91%. misses: 285\n",
            "eval: split test . loss 1.146416e-01. error 3.54%. misses: 71\n",
            "epoch 14 with learning rate 0.000267\n",
            "eval: split train. loss 1.418008e-01. error 4.39%. misses: 320\n",
            "eval: split test . loss 1.211559e-01. error 3.44%. misses: 69\n",
            "epoch 15 with learning rate 0.000265\n",
            "eval: split train. loss 1.200302e-01. error 3.83%. misses: 279\n",
            "eval: split test . loss 1.037141e-01. error 3.04%. misses: 61\n",
            "epoch 16 with learning rate 0.000262\n",
            "eval: split train. loss 1.152942e-01. error 3.58%. misses: 261\n",
            "eval: split test . loss 9.243891e-02. error 2.84%. misses: 57\n",
            "epoch 17 with learning rate 0.000260\n",
            "eval: split train. loss 1.114711e-01. error 3.36%. misses: 245\n",
            "eval: split test . loss 9.784427e-02. error 2.79%. misses: 56\n",
            "epoch 18 with learning rate 0.000258\n",
            "eval: split train. loss 1.079472e-01. error 3.26%. misses: 237\n",
            "eval: split test . loss 9.607720e-02. error 3.29%. misses: 66\n",
            "epoch 19 with learning rate 0.000255\n",
            "eval: split train. loss 1.044038e-01. error 3.33%. misses: 242\n",
            "eval: split test . loss 9.272417e-02. error 2.84%. misses: 57\n",
            "epoch 20 with learning rate 0.000253\n",
            "eval: split train. loss 1.071771e-01. error 3.32%. misses: 242\n",
            "eval: split test . loss 9.934853e-02. error 3.54%. misses: 71\n",
            "epoch 21 with learning rate 0.000250\n",
            "eval: split train. loss 9.646057e-02. error 2.72%. misses: 198\n",
            "eval: split test . loss 9.175012e-02. error 2.79%. misses: 56\n",
            "epoch 22 with learning rate 0.000247\n",
            "eval: split train. loss 1.005183e-01. error 2.91%. misses: 212\n",
            "eval: split test . loss 9.100603e-02. error 3.09%. misses: 62\n",
            "epoch 23 with learning rate 0.000245\n",
            "eval: split train. loss 9.477063e-02. error 2.83%. misses: 206\n",
            "eval: split test . loss 8.572441e-02. error 2.64%. misses: 53\n",
            "epoch 24 with learning rate 0.000242\n",
            "eval: split train. loss 9.666386e-02. error 2.91%. misses: 212\n",
            "eval: split test . loss 9.094071e-02. error 2.89%. misses: 58\n",
            "epoch 25 with learning rate 0.000240\n",
            "eval: split train. loss 9.753828e-02. error 3.02%. misses: 220\n",
            "eval: split test . loss 8.659974e-02. error 2.79%. misses: 56\n",
            "epoch 26 with learning rate 0.000238\n",
            "eval: split train. loss 8.909303e-02. error 2.92%. misses: 213\n",
            "eval: split test . loss 9.348330e-02. error 2.99%. misses: 60\n",
            "epoch 27 with learning rate 0.000235\n",
            "eval: split train. loss 8.077087e-02. error 2.43%. misses: 177\n",
            "eval: split test . loss 7.055859e-02. error 1.94%. misses: 39\n",
            "epoch 28 with learning rate 0.000232\n",
            "eval: split train. loss 8.742411e-02. error 2.84%. misses: 207\n",
            "eval: split test . loss 8.102497e-02. error 2.49%. misses: 50\n",
            "epoch 29 with learning rate 0.000230\n",
            "eval: split train. loss 8.343960e-02. error 2.48%. misses: 181\n",
            "eval: split test . loss 7.679689e-02. error 2.24%. misses: 45\n",
            "epoch 30 with learning rate 0.000227\n",
            "eval: split train. loss 9.004340e-02. error 2.78%. misses: 203\n",
            "eval: split test . loss 8.523306e-02. error 2.49%. misses: 50\n",
            "epoch 31 with learning rate 0.000225\n",
            "eval: split train. loss 7.902195e-02. error 2.55%. misses: 186\n",
            "eval: split test . loss 7.678007e-02. error 2.24%. misses: 45\n",
            "epoch 32 with learning rate 0.000223\n",
            "eval: split train. loss 7.674854e-02. error 2.37%. misses: 173\n",
            "eval: split test . loss 7.416118e-02. error 2.54%. misses: 51\n",
            "epoch 33 with learning rate 0.000220\n",
            "eval: split train. loss 8.023622e-02. error 2.54%. misses: 185\n",
            "eval: split test . loss 7.356358e-02. error 2.14%. misses: 43\n",
            "epoch 34 with learning rate 0.000217\n",
            "eval: split train. loss 8.085081e-02. error 2.50%. misses: 182\n",
            "eval: split test . loss 8.324280e-02. error 2.64%. misses: 53\n",
            "epoch 35 with learning rate 0.000215\n",
            "eval: split train. loss 7.202547e-02. error 2.51%. misses: 183\n",
            "eval: split test . loss 7.461900e-02. error 2.29%. misses: 46\n",
            "epoch 36 with learning rate 0.000212\n",
            "eval: split train. loss 7.868081e-02. error 2.26%. misses: 164\n",
            "eval: split test . loss 7.758510e-02. error 2.44%. misses: 49\n",
            "epoch 37 with learning rate 0.000210\n",
            "eval: split train. loss 7.236929e-02. error 2.22%. misses: 162\n",
            "eval: split test . loss 6.601710e-02. error 1.94%. misses: 39\n",
            "epoch 38 with learning rate 0.000208\n",
            "eval: split train. loss 7.216260e-02. error 2.24%. misses: 163\n",
            "eval: split test . loss 6.878507e-02. error 2.04%. misses: 41\n",
            "epoch 39 with learning rate 0.000205\n",
            "eval: split train. loss 7.536543e-02. error 2.48%. misses: 181\n",
            "eval: split test . loss 7.363439e-02. error 2.39%. misses: 48\n",
            "epoch 40 with learning rate 0.000202\n",
            "eval: split train. loss 7.205240e-02. error 2.25%. misses: 164\n",
            "eval: split test . loss 7.575999e-02. error 2.19%. misses: 44\n",
            "epoch 41 with learning rate 0.000200\n",
            "eval: split train. loss 8.093227e-02. error 2.44%. misses: 178\n",
            "eval: split test . loss 7.694457e-02. error 2.49%. misses: 50\n",
            "epoch 42 with learning rate 0.000197\n",
            "eval: split train. loss 6.429646e-02. error 1.95%. misses: 142\n",
            "eval: split test . loss 6.771988e-02. error 2.04%. misses: 41\n",
            "epoch 43 with learning rate 0.000195\n",
            "eval: split train. loss 6.339417e-02. error 1.98%. misses: 144\n",
            "eval: split test . loss 6.691701e-02. error 1.79%. misses: 36\n",
            "epoch 44 with learning rate 0.000192\n",
            "eval: split train. loss 6.447195e-02. error 1.92%. misses: 140\n",
            "eval: split test . loss 6.421570e-02. error 1.89%. misses: 38\n",
            "epoch 45 with learning rate 0.000190\n",
            "eval: split train. loss 7.070693e-02. error 2.13%. misses: 155\n",
            "eval: split test . loss 6.277889e-02. error 1.79%. misses: 36\n",
            "epoch 46 with learning rate 0.000188\n",
            "eval: split train. loss 6.487084e-02. error 1.91%. misses: 139\n",
            "eval: split test . loss 6.121965e-02. error 1.79%. misses: 36\n",
            "epoch 47 with learning rate 0.000185\n",
            "eval: split train. loss 6.111839e-02. error 1.93%. misses: 141\n",
            "eval: split test . loss 6.361532e-02. error 2.04%. misses: 41\n",
            "epoch 48 with learning rate 0.000182\n",
            "eval: split train. loss 6.650443e-02. error 2.11%. misses: 154\n",
            "eval: split test . loss 6.411713e-02. error 1.99%. misses: 40\n",
            "epoch 49 with learning rate 0.000180\n",
            "eval: split train. loss 7.477588e-02. error 2.51%. misses: 183\n",
            "eval: split test . loss 8.306032e-02. error 2.59%. misses: 52\n",
            "epoch 50 with learning rate 0.000177\n",
            "eval: split train. loss 5.802518e-02. error 1.74%. misses: 126\n",
            "eval: split test . loss 6.448522e-02. error 2.04%. misses: 41\n",
            "epoch 51 with learning rate 0.000175\n",
            "eval: split train. loss 6.023614e-02. error 1.77%. misses: 129\n",
            "eval: split test . loss 6.091921e-02. error 1.99%. misses: 40\n",
            "epoch 52 with learning rate 0.000173\n",
            "eval: split train. loss 5.690415e-02. error 1.65%. misses: 120\n",
            "eval: split test . loss 5.874708e-02. error 1.89%. misses: 38\n",
            "epoch 53 with learning rate 0.000170\n",
            "eval: split train. loss 5.996694e-02. error 1.91%. misses: 139\n",
            "eval: split test . loss 6.767343e-02. error 2.39%. misses: 48\n",
            "epoch 54 with learning rate 0.000167\n",
            "eval: split train. loss 6.559739e-02. error 2.00%. misses: 146\n",
            "eval: split test . loss 6.906316e-02. error 2.44%. misses: 49\n",
            "epoch 55 with learning rate 0.000165\n",
            "eval: split train. loss 5.651071e-02. error 1.81%. misses: 132\n",
            "eval: split test . loss 6.128595e-02. error 1.74%. misses: 35\n",
            "epoch 56 with learning rate 0.000162\n",
            "eval: split train. loss 6.510165e-02. error 2.04%. misses: 149\n",
            "eval: split test . loss 7.154531e-02. error 2.59%. misses: 52\n",
            "epoch 57 with learning rate 0.000160\n",
            "eval: split train. loss 5.994103e-02. error 1.96%. misses: 143\n",
            "eval: split test . loss 6.686199e-02. error 2.04%. misses: 41\n",
            "epoch 58 with learning rate 0.000158\n",
            "eval: split train. loss 6.084277e-02. error 1.99%. misses: 145\n",
            "eval: split test . loss 6.122357e-02. error 2.04%. misses: 41\n",
            "epoch 59 with learning rate 0.000155\n",
            "eval: split train. loss 5.618722e-02. error 1.81%. misses: 132\n",
            "eval: split test . loss 6.144323e-02. error 2.09%. misses: 42\n",
            "epoch 60 with learning rate 0.000152\n",
            "eval: split train. loss 6.152606e-02. error 2.02%. misses: 147\n",
            "eval: split test . loss 7.784808e-02. error 2.59%. misses: 52\n",
            "epoch 61 with learning rate 0.000150\n",
            "eval: split train. loss 4.968056e-02. error 1.48%. misses: 108\n",
            "eval: split test . loss 6.235341e-02. error 1.84%. misses: 37\n",
            "epoch 62 with learning rate 0.000148\n",
            "eval: split train. loss 4.810822e-02. error 1.37%. misses: 99\n",
            "eval: split test . loss 5.564752e-02. error 1.59%. misses: 32\n",
            "epoch 63 with learning rate 0.000145\n",
            "eval: split train. loss 5.637766e-02. error 1.85%. misses: 135\n",
            "eval: split test . loss 6.604350e-02. error 2.09%. misses: 42\n",
            "epoch 64 with learning rate 0.000142\n",
            "eval: split train. loss 5.689941e-02. error 1.92%. misses: 140\n",
            "eval: split test . loss 6.196598e-02. error 2.14%. misses: 43\n",
            "epoch 65 with learning rate 0.000140\n",
            "eval: split train. loss 4.938858e-02. error 1.47%. misses: 107\n",
            "eval: split test . loss 5.717899e-02. error 1.74%. misses: 35\n",
            "epoch 66 with learning rate 0.000137\n",
            "eval: split train. loss 4.930802e-02. error 1.48%. misses: 108\n",
            "eval: split test . loss 6.262989e-02. error 1.84%. misses: 37\n",
            "epoch 67 with learning rate 0.000135\n",
            "eval: split train. loss 5.547680e-02. error 1.84%. misses: 133\n",
            "eval: split test . loss 6.177986e-02. error 1.94%. misses: 39\n",
            "epoch 68 with learning rate 0.000133\n",
            "eval: split train. loss 4.687589e-02. error 1.40%. misses: 101\n",
            "eval: split test . loss 5.382073e-02. error 1.44%. misses: 29\n",
            "epoch 69 with learning rate 0.000130\n",
            "eval: split train. loss 4.964928e-02. error 1.59%. misses: 116\n",
            "eval: split test . loss 5.405175e-02. error 1.74%. misses: 35\n",
            "epoch 70 with learning rate 0.000127\n",
            "eval: split train. loss 5.028207e-02. error 1.47%. misses: 107\n",
            "eval: split test . loss 6.269918e-02. error 1.89%. misses: 38\n",
            "epoch 71 with learning rate 0.000125\n",
            "eval: split train. loss 5.439356e-02. error 1.71%. misses: 125\n",
            "eval: split test . loss 5.837831e-02. error 1.94%. misses: 39\n",
            "epoch 72 with learning rate 0.000123\n",
            "eval: split train. loss 5.272948e-02. error 1.62%. misses: 118\n",
            "eval: split test . loss 6.305528e-02. error 2.19%. misses: 44\n",
            "epoch 73 with learning rate 0.000120\n",
            "eval: split train. loss 4.887568e-02. error 1.48%. misses: 108\n",
            "eval: split test . loss 5.599349e-02. error 1.89%. misses: 38\n",
            "epoch 74 with learning rate 0.000117\n",
            "eval: split train. loss 4.862784e-02. error 1.55%. misses: 113\n",
            "eval: split test . loss 5.785608e-02. error 1.89%. misses: 38\n",
            "epoch 75 with learning rate 0.000115\n",
            "eval: split train. loss 4.730204e-02. error 1.55%. misses: 113\n",
            "eval: split test . loss 5.783835e-02. error 1.89%. misses: 38\n",
            "epoch 76 with learning rate 0.000112\n",
            "eval: split train. loss 4.385144e-02. error 1.29%. misses: 94\n",
            "eval: split test . loss 5.820187e-02. error 2.09%. misses: 42\n",
            "epoch 77 with learning rate 0.000110\n",
            "eval: split train. loss 4.882991e-02. error 1.58%. misses: 114\n",
            "eval: split test . loss 5.734961e-02. error 1.89%. misses: 38\n",
            "epoch 78 with learning rate 0.000108\n",
            "eval: split train. loss 4.475906e-02. error 1.45%. misses: 106\n",
            "eval: split test . loss 5.110974e-02. error 1.84%. misses: 37\n",
            "epoch 79 with learning rate 0.000105\n",
            "eval: split train. loss 4.341912e-02. error 1.39%. misses: 101\n",
            "eval: split test . loss 5.870167e-02. error 2.14%. misses: 43\n",
            "epoch 80 with learning rate 0.000102\n",
            "eval: split train. loss 4.375895e-02. error 1.34%. misses: 97\n",
            "eval: split test . loss 5.678927e-02. error 1.79%. misses: 36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change 1: replace tanh on last layer with FC and use softmax. Lower learning rate to 0.01\n",
        "\n",
        "```\n",
        "epoch 23\n",
        "eval: split train. loss 7.162272e-03. error 0.05%. misses: 4\n",
        "eval: split test . loss 1.687743e-01. error 4.14%. misses: 83\n",
        "\n",
        "```\n",
        "\n",
        "Change 2: change from SGD to AdamW with LR 3e-4, double epochs to 46, decay LR to 1e-4 over the course of training.\n",
        "\n",
        "```\n",
        "epoch 46\n",
        "eval: split train. loss 1.890260e-03. error 0.04%. misses: 2\n",
        "eval: split test . loss 1.953933e-01. error 4.04%. misses: 81\n",
        "```\n",
        "\n",
        "Change 3: Introduce data augmentation, e.g. a shift by at most 1 pixel in both x/y directions, and bump up training time to 60 epochs.\n",
        "\n",
        "```\n",
        "epoch 60\n",
        "eval: split train. loss 5.098452e-02. error 1.65%. misses: 120\n",
        "eval: split test . loss 9.166716e-02. error 2.59%. misses: 52\n",
        "```\n",
        "\n",
        "Change 4: add dropout at layer H3, shift activation function to relu, and bring up iterations to 80.\n",
        "\n",
        "```\n",
        "epoch 80\n",
        "eval: split train. loss 4.375895e-02. error 1.34%. misses: 97\n",
        "eval: split test . loss 5.678927e-02. error 1.79%. misses: 36\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8Ey5_RPUytFV"
      }
    }
  ]
}