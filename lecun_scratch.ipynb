{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lecun_scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peterchang0414/lecun1989-flax/blob/main/lecun_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1989 Reproduction"
      ],
      "metadata": {
        "id": "N7JVDrbphkxe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oe2o-uZZd4yT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58310a6f-e0dc-457e-9ba3-55558ed9b210"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 20.2 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 20 kB 18.1 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 30 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 40 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 51 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 61 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 71 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 81 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 92 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 102 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 112 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 122 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 133 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 143 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 153 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 163 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 174 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 184 kB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 136 kB 49.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 70 kB 6.6 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q flax"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "from flax import linen as nn\n",
        "from torchvision import datasets\n",
        "\n",
        "# Adapted from https://github.com/karpathy/lecun1989-repro/blob/master/prepro.py\n",
        "def get_datasets(n_tr, n_te):\n",
        "  train_test = {}\n",
        "  for split in {'train', 'test'}:\n",
        "    data = datasets.MNIST('./data', train=split=='train', download=True)\n",
        "    \n",
        "    n = n_tr if split == 'train' else n_te\n",
        "    key = jax.random.PRNGKey(42)\n",
        "    rp = jax.random.permutation(key, len(data))[:n]\n",
        "\n",
        "    X = jnp.full((n, 16, 16, 1), 0.0, dtype=jnp.float32)\n",
        "    Y = jnp.full((n, 10), -1.0, dtype=jnp.float32)\n",
        "    for i, ix in enumerate(rp):\n",
        "      I, yint = data[int(ix)]\n",
        "      xi = jnp.array(I, dtype=np.float32) / 127.5 - 1.0\n",
        "      xi = jax.image.resize(xi, (16, 16), 'bilinear')\n",
        "      X = X.at[i].set(np.expand_dims(xi, axis=2))\n",
        "      Y = Y.at[i, yint].set(1.0)\n",
        "    train_test[split] = (X, Y)\n",
        "  return train_test"
      ],
      "metadata": {
        "id": "pc7McMNRnt2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flax import linen as nn\n",
        "from flax.training import train_state\n",
        "from flax.linen.activation import tanh\n",
        "import optax\n",
        "from typing import Callable\n",
        "\n",
        "class Net(nn.Module):\n",
        "  bias_init: Callable = nn.initializers.zeros\n",
        "  kernel_init: Callable = nn.initializers.uniform()\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    # For weight initialization, Karpathy used numerator of 2.4 \n",
        "    # which is very close to sqrt(6) = 2.449... used by he_uniform()\n",
        "    # By default, weight-sharing forces bias-sharing and therefore\n",
        "    # we add the bias separately.\n",
        "    x = jnp.pad(x, [(0,0),(2,2),(2,2),(0,0)], constant_values=-1.0)\n",
        "    x = nn.Conv(features=12, kernel_size=(5,5), strides=2, padding='VALID',\n",
        "                use_bias=False, kernel_init=self.kernel_init)(x)\n",
        "    bias1 = self.param('bias1', self.bias_init, (8, 8, 12))\n",
        "    x = tanh(x + bias1)\n",
        "    x = jnp.pad(x, [(0,0),(2,2),(2,2),(0,0)], constant_values=-1.0)\n",
        "    x1, x2, x3 = (x[..., 0:8], x[..., 4:12], \n",
        "                  jnp.concatenate((x[..., 0:4], x[..., 8:12]), axis=-1))\n",
        "    slice1 = nn.Conv(features=4, kernel_size=(5,5), strides=2, padding='VALID', \n",
        "                     use_bias=False, kernel_init=self.kernel_init)(x1)\n",
        "    slice2 = nn.Conv(features=4, kernel_size=(5,5), strides=2, padding='VALID',\n",
        "                     use_bias=False, kernel_init=self.kernel_init)(x2)\n",
        "    slice3 = nn.Conv(features=4, kernel_size=(5,5), strides=2, padding='VALID',\n",
        "                     use_bias=False, kernel_init=self.kernel_init)(x3)\n",
        "    x = jnp.concatenate((slice1, slice2, slice3), axis=-1)\n",
        "    bias2 = self.param('bias2', self.bias_init, (4, 4, 12))\n",
        "    x = tanh(x + bias2)\n",
        "    x = x.reshape((x.shape[0], -1))\n",
        "    x = nn.Dense(features=30, use_bias=False)(x)\n",
        "    bias3 = self.param('bias3', self.bias_init, (30,))\n",
        "    x = tanh(x + bias3)\n",
        "    x = nn.Dense(features=10, use_bias=False)(x)\n",
        "    bias4 = self.param('bias4', nn.initializers.constant(-1.0), (10,))\n",
        "    x = tanh(x + bias4)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "hk3obOlvqoDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def eval_step(params, X, Y):\n",
        "  Yhat = Net().apply({'params': params}, X)\n",
        "  loss = jnp.mean((Yhat - Y)**2)\n",
        "  err = jnp.mean(jnp.argmax(Y, -1) != jnp.argmax(Yhat, -1)).astype(float)\n",
        "  return loss, err"
      ],
      "metadata": {
        "id": "KUCi-jvDXuha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_split(data, split, params):\n",
        "  X, Y = data[split]\n",
        "  loss, err = eval_step(params, X, Y)\n",
        "  print(f\"eval: split {split:5s}. loss {loss:e}. error {err*100:.2f}%. misses: {int(err*Y.shape[0])}\")"
      ],
      "metadata": {
        "id": "D9KDZtpgVUBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import value_and_grad\n",
        "import optax\n",
        "from flax.training import train_state\n",
        "\n",
        "def create_train_state(key, lr, X):\n",
        "  model = Net()\n",
        "  params = model.init(key, X)['params']\n",
        "  sgd_opt = optax.sgd(lr)\n",
        "  return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=sgd_opt)\n",
        "\n",
        "@jax.jit\n",
        "def train_step(state, X, Y):\n",
        "  def loss_fn(params):\n",
        "    Yhat = Net().apply({'params': params}, X)\n",
        "    loss = jnp.mean((Yhat - Y)**2)\n",
        "    err = jnp.mean(jnp.argmax(Y, -1) != jnp.argmax(Yhat, -1)).astype(float)\n",
        "    return loss, err\n",
        "  (_, Yhats), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
        "  state = state.apply_gradients(grads=grads)\n",
        "  return state\n",
        "\n",
        "def train_one_epoch(state, X, Y):\n",
        "  for step_num in range(X.shape[0]):\n",
        "    x, y = jnp.expand_dims(X[step_num], 0), jnp.expand_dims(Y[step_num], 0)\n",
        "    state = train_step(state, x, y)\n",
        "  return state\n",
        "\n",
        "def train(key, data, epochs, lr):\n",
        "  Xtr, Ytr = data['train']\n",
        "  Xte, Yte = data['test']\n",
        "  train_state = create_train_state(key, lr, Xtr)\n",
        "  for epoch in range(epochs):\n",
        "    print(f\"epoch {epoch+1}\")\n",
        "    train_state = train_one_epoch(train_state, Xtr, Ytr)\n",
        "    for split in ['train', 'test']:\n",
        "      eval_split(data, split, train_state.params)\n"
      ],
      "metadata": {
        "id": "nwzevZv3CdvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = get_datasets(7291, 2007)"
      ],
      "metadata": {
        "id": "phiwVJtuFerc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key, _ = jax.random.split(jax.random.PRNGKey(42))\n",
        "\n",
        "train(key, data, 23, 0.03)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "lZhSr9-YKlo5",
        "outputId": "19a3315e-398c-4b10-e318-5e5205f8051a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-cb9467ddae03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7291\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2007\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m23\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.03\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-6c765f3c17c8>\u001b[0m in \u001b[0;36mget_datasets\u001b[0;34m(n_tr, n_te)\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0mxi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m127.5\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[0mxi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m       \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m       \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mtrain_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, values, indices_are_sorted, unique_indices, mode)\u001b[0m\n\u001b[1;32m   5523\u001b[0m     return scatter._scatter_update(self.array, self.index, values, lax.scatter,\n\u001b[1;32m   5524\u001b[0m                                    \u001b[0mindices_are_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices_are_sorted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5525\u001b[0;31m                                    unique_indices=unique_indices, mode=mode)\n\u001b[0m\u001b[1;32m   5526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5527\u001b[0m   def apply(self, func, indices_are_sorted=False, unique_indices=False,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/ops/scatter.py\u001b[0m in \u001b[0;36m_scatter_update\u001b[0;34m(x, idx, y, scatter_op, indices_are_sorted, unique_indices, mode, normalize_indices)\u001b[0m\n\u001b[1;32m     70\u001b[0m   return _scatter_impl(x, y, scatter_op, treedef, static_idx, dynamic_idx,\n\u001b[1;32m     71\u001b[0m                        \u001b[0mindices_are_sorted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                        normalize_indices)\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/ops/scatter.py\u001b[0m in \u001b[0;36m_scatter_impl\u001b[0;34m(x, y, scatter_op, treedef, static_idx, dynamic_idx, indices_are_sorted, unique_indices, mode, normalize_indices)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mindices_are_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices_are_sorted\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindices_are_sorted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0munique_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_indices\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0munique_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     mode=mode)\n\u001b[0m\u001b[1;32m    114\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mlax_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_element_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweak_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/lax/slicing.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(operand, scatter_indices, updates, dimension_numbers, indices_are_sorted, unique_indices, mode)\u001b[0m\n\u001b[1;32m    603\u001b[0m       \u001b[0mupdate_consts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconsts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimension_numbers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdimension_numbers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m       \u001b[0mindices_are_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices_are_sorted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munique_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m       mode=GatherScatterMode.from_any(mode))\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mindex_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    284\u001b[0m     assert (not config.jax_enable_checks or\n\u001b[1;32m    285\u001b[0m             all(isinstance(arg, Tracer) or valid_jaxtype(arg) for arg in args)), args\n\u001b[0;32m--> 286\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_top_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mbind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfull_lower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mprocess_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/dispatch.py\u001b[0m in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m     92\u001b[0m   compiled_fun = xla_primitive_callable(prim, *unsafe_map(arg_spec, args),\n\u001b[1;32m     93\u001b[0m                                         **params)\n\u001b[0;32m---> 94\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;31m# TODO(phawkins): update code referring to xla.apply_primitive to point here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/dispatch.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    112\u001b[0m                                     prim.name, donated_invars, *arg_specs)\n\u001b[1;32m    113\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mprim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/dispatch.py\u001b[0m in \u001b[0;36m_execute_compiled\u001b[0;34m(name, compiled, output_buffer_counts, result_handlers, kept_var_idx, *args)\u001b[0m\n\u001b[1;32m    442\u001b[0m   input_bufs = util.flatten(\n\u001b[1;32m    443\u001b[0m       device_put(x, device) for i, x in enumerate(args) if i in kept_var_idx)\n\u001b[0;32m--> 444\u001b[0;31m   \u001b[0mout_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m   \u001b[0mcheck_special\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0moutput_buffer_counts\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results:\n",
        "\n",
        "```\n",
        "epoch 23\n",
        "eval: split train. loss 6.397844e-03. error 0.97%. misses: 71\n",
        "eval: split test . loss 2.608742e-02. error 3.84%. misses: 77\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "rOaujFXQyqjd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# \"Modern\" Adjustments"
      ],
      "metadata": {
        "id": "cNEUr0vSfh1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q flax"
      ],
      "metadata": {
        "id": "vLDKA_qjhy6S",
        "outputId": "dd1981ee-1c4f-41c1-e827-54898dc8af99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 20 kB 12.7 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 30 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 40 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 51 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 61 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 71 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 81 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 92 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 102 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 112 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 122 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 133 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 143 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 153 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 163 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 174 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 184 kB 5.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 136 kB 64.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 70 kB 8.7 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "from flax import linen as nn\n",
        "from torchvision import datasets\n",
        "\n",
        "# Adapted from https://github.com/karpathy/lecun1989-repro/blob/master/prepro.py\n",
        "def get_datasets(n_tr, n_te):\n",
        "  train_test = {}\n",
        "  for split in {'train', 'test'}:\n",
        "    data = datasets.MNIST('./data', train=split=='train', download=True)\n",
        "    \n",
        "    n = n_tr if split == 'train' else n_te\n",
        "    key = jax.random.PRNGKey(42)\n",
        "    rp = jax.random.permutation(key, len(data))[:n]\n",
        "\n",
        "    X = jnp.full((n, 16, 16, 1), 0.0, dtype=jnp.float32)\n",
        "    Y = jnp.full((n, 10), 0, dtype=jnp.float32)\n",
        "    for i, ix in enumerate(rp):\n",
        "      I, yint = data[int(ix)]\n",
        "      xi = jnp.array(I, dtype=np.float32) / 127.5 - 1.0\n",
        "      xi = jax.image.resize(xi, (16, 16), 'bilinear')\n",
        "      X = X.at[i].set(np.expand_dims(xi, axis=2))\n",
        "      Y = Y.at[i, yint].set(1.0)\n",
        "    train_test[split] = (X, Y)\n",
        "  return train_test"
      ],
      "metadata": {
        "id": "m68J20OSh0rh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flax import linen as nn\n",
        "from flax.training import train_state\n",
        "from flax.linen.activation import tanh\n",
        "import optax\n",
        "from typing import Callable\n",
        "\n",
        "class Net(nn.Module):\n",
        "  training: bool\n",
        "  bias_init: Callable = nn.initializers.zeros\n",
        "  kernel_init: Callable = nn.initializers.uniform()\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    # For weight initialization, Karpathy used numerator of 2.4 \n",
        "    # which is very close to sqrt(6) = 2.449... used by he_uniform()\n",
        "    # By default, weight-sharing forces bias-sharing and therefore\n",
        "    # we add the bias separately.\n",
        "    if self.training:\n",
        "      augment_rng = self.make_rng('aug')\n",
        "      # self.sow('intermediates', 'aug_rng', augment_rng)\n",
        "      shift_x, shift_y = jax.random.randint(augment_rng, (2,), -1, 2)\n",
        "      x = jnp.roll(x, (shift_x, shift_y), (1, 2))\n",
        "\n",
        "    x = jnp.pad(x, [(0,0),(2,2),(2,2),(0,0)], constant_values=-1.0)\n",
        "    x = nn.Conv(features=12, kernel_size=(5,5), strides=2, padding='VALID',\n",
        "                use_bias=False, kernel_init=self.kernel_init)(x)\n",
        "    bias1 = self.param('bias1', self.bias_init, (8, 8, 12))\n",
        "    x = tanh(x + bias1)\n",
        "    x = jnp.pad(x, [(0,0),(2,2),(2,2),(0,0)], constant_values=-1.0)\n",
        "    x1, x2, x3 = (x[..., 0:8], x[..., 4:12], \n",
        "                  jnp.concatenate((x[..., 0:4], x[..., 8:12]), axis=-1))\n",
        "    slice1 = nn.Conv(features=4, kernel_size=(5,5), strides=2, padding='VALID', \n",
        "                     use_bias=False, kernel_init=self.kernel_init)(x1)\n",
        "    slice2 = nn.Conv(features=4, kernel_size=(5,5), strides=2, padding='VALID',\n",
        "                     use_bias=False, kernel_init=self.kernel_init)(x2)\n",
        "    slice3 = nn.Conv(features=4, kernel_size=(5,5), strides=2, padding='VALID',\n",
        "                     use_bias=False, kernel_init=self.kernel_init)(x3)\n",
        "    x = jnp.concatenate((slice1, slice2, slice3), axis=-1)\n",
        "    bias2 = self.param('bias2', self.bias_init, (4, 4, 12))\n",
        "    x = tanh(x + bias2)\n",
        "    x = x.reshape((x.shape[0], -1))\n",
        "    x = nn.Dense(features=30, use_bias=False)(x)\n",
        "    bias3 = self.param('bias3', self.bias_init, (30,))\n",
        "    x = tanh(x + bias3)\n",
        "    x = nn.Dense(features=10, use_bias=False)(x)\n",
        "    bias4 = self.param('bias4', nn.initializers.constant(-1.0), (10,))\n",
        "    x = x + bias4\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "voQJfHf6thIH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import value_and_grad\n",
        "import optax\n",
        "from flax.training import train_state\n",
        "\n",
        "def learning_rate_fn(initial_rate, epochs, steps_per_epoch):\n",
        "  return optax.linear_schedule(init_value=initial_rate, end_value=initial_rate/3,\n",
        "                               transition_steps=epochs*steps_per_epoch)\n",
        "\n",
        "def create_train_state(key, X, lr_fn):\n",
        "  model = Net(training=True)\n",
        "  key1, key2 = jax.random.split(key)\n",
        "  params = model.init({'params': key1, 'aug': key2}, X)['params']\n",
        "  opt = optax.adamw(lr_fn)\n",
        "  return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=opt)\n",
        "\n",
        "@jax.jit\n",
        "def train_step(state, X, Y, aug_rng=jax.random.PRNGKey(0),\n",
        "               dropout_rng=jax.random.PRNGKey(0)):\n",
        "  aug_rng = jax.random.fold_in(aug_rng, state.step)\n",
        "  dropout_rng, _ = jax.random.split(aug_rng)\n",
        "  def loss_fn(params):\n",
        "    Yhat = Net(training=True).apply({'params': params}, X, \n",
        "                                           rngs={'aug': aug_rng})\n",
        "    loss = jnp.mean(optax.softmax_cross_entropy(logits=Yhat, labels=Y))\n",
        "    err = jnp.mean(jnp.argmax(Y, -1) != jnp.argmax(Yhat, -1)).astype(float)\n",
        "    return loss, err\n",
        "  (_, Yhats), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
        "  state = state.apply_gradients(grads=grads)\n",
        "  return state\n",
        "\n",
        "def train_one_epoch(state, X, Y):\n",
        "  for step_num in range(X.shape[0]):\n",
        "    x, y = jnp.expand_dims(X[step_num], 0), jnp.expand_dims(Y[step_num], 0)\n",
        "    state = train_step(state, x, y)\n",
        "  return state\n",
        "\n",
        "def train(key, data, epochs, lr):\n",
        "  Xtr, Ytr = data['train']\n",
        "  Xte, Yte = data['test']\n",
        "  lr_fn = learning_rate_fn(lr, epochs, Xtr.shape[0])\n",
        "  train_state = create_train_state(key, Xtr, lr_fn)\n",
        "  for epoch in range(epochs):\n",
        "    print(f\"epoch {epoch+1} with learning rate {lr_fn(train_state.step):.6f}\")\n",
        "    train_state = train_one_epoch(train_state, Xtr, Ytr)\n",
        "    for split in ['train', 'test']:\n",
        "      eval_split(data, split, train_state.params)\n"
      ],
      "metadata": {
        "id": "emEVm2HViete"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def eval_step(params, X, Y):\n",
        "  Yhat = Net(training=False).apply({'params': params}, X)\n",
        "  loss = jnp.mean(optax.softmax_cross_entropy(logits=Yhat, labels=Y))\n",
        "  err = jnp.mean(jnp.argmax(Y, -1) != jnp.argmax(Yhat, -1)).astype(float)\n",
        "  return loss, err\n",
        "\n",
        "def eval_split(data, split, params):\n",
        "  X, Y = data[split]\n",
        "  loss, err = eval_step(params, X, Y)\n",
        "  print(f\"eval: split {split:5s}. loss {loss:e}. error {err*100:.2f}%. misses: {int(err*Y.shape[0])}\")"
      ],
      "metadata": {
        "id": "fRywm7o5jxPy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = get_datasets(7291, 2007)"
      ],
      "metadata": {
        "id": "G5D2wrzjEayd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key, _ = jax.random.split(jax.random.PRNGKey(42))\n",
        "\n",
        "train(key, data, 60, 3e-4)"
      ],
      "metadata": {
        "id": "2vncyC3wohuc",
        "outputId": "44efe5f6-0500-440d-c1f6-4efb3bda26aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 with learning rate 0.000300\n",
            "eval: split train. loss 5.425038e-01. error 13.44%. misses: 980\n",
            "eval: split test . loss 5.180030e-01. error 12.71%. misses: 255\n",
            "epoch 2 with learning rate 0.000297\n",
            "eval: split train. loss 3.860081e-01. error 10.52%. misses: 767\n",
            "eval: split test . loss 3.717132e-01. error 10.31%. misses: 207\n",
            "epoch 3 with learning rate 0.000293\n",
            "eval: split train. loss 3.163782e-01. error 9.22%. misses: 672\n",
            "eval: split test . loss 3.055812e-01. error 9.47%. misses: 190\n",
            "epoch 4 with learning rate 0.000290\n",
            "eval: split train. loss 2.549224e-01. error 7.43%. misses: 542\n",
            "eval: split test . loss 2.393445e-01. error 6.38%. misses: 128\n",
            "epoch 5 with learning rate 0.000287\n",
            "eval: split train. loss 2.062259e-01. error 6.13%. misses: 447\n",
            "eval: split test . loss 1.851927e-01. error 5.28%. misses: 106\n",
            "epoch 6 with learning rate 0.000283\n",
            "eval: split train. loss 1.817566e-01. error 5.35%. misses: 390\n",
            "eval: split test . loss 1.699917e-01. error 5.28%. misses: 106\n",
            "epoch 7 with learning rate 0.000280\n",
            "eval: split train. loss 1.696782e-01. error 5.07%. misses: 370\n",
            "eval: split test . loss 1.598778e-01. error 4.38%. misses: 88\n",
            "epoch 8 with learning rate 0.000277\n",
            "eval: split train. loss 1.539604e-01. error 4.87%. misses: 355\n",
            "eval: split test . loss 1.460883e-01. error 4.68%. misses: 94\n",
            "epoch 9 with learning rate 0.000273\n",
            "eval: split train. loss 1.587229e-01. error 4.90%. misses: 356\n",
            "eval: split test . loss 1.556241e-01. error 4.83%. misses: 97\n",
            "epoch 10 with learning rate 0.000270\n",
            "eval: split train. loss 1.403436e-01. error 4.42%. misses: 321\n",
            "eval: split test . loss 1.348723e-01. error 4.19%. misses: 84\n",
            "epoch 11 with learning rate 0.000267\n",
            "eval: split train. loss 1.370205e-01. error 4.17%. misses: 304\n",
            "eval: split test . loss 1.337586e-01. error 4.38%. misses: 88\n",
            "epoch 12 with learning rate 0.000263\n",
            "eval: split train. loss 1.340350e-01. error 4.05%. misses: 294\n",
            "eval: split test . loss 1.347075e-01. error 4.58%. misses: 92\n",
            "epoch 13 with learning rate 0.000260\n",
            "eval: split train. loss 1.397673e-01. error 4.27%. misses: 311\n",
            "eval: split test . loss 1.455714e-01. error 4.63%. misses: 93\n",
            "epoch 14 with learning rate 0.000257\n",
            "eval: split train. loss 1.256178e-01. error 3.57%. misses: 259\n",
            "eval: split test . loss 1.349332e-01. error 4.09%. misses: 82\n",
            "epoch 15 with learning rate 0.000253\n",
            "eval: split train. loss 1.292990e-01. error 3.98%. misses: 290\n",
            "eval: split test . loss 1.260124e-01. error 4.04%. misses: 81\n",
            "epoch 16 with learning rate 0.000250\n",
            "eval: split train. loss 1.184599e-01. error 3.62%. misses: 264\n",
            "eval: split test . loss 1.270302e-01. error 3.84%. misses: 77\n",
            "epoch 17 with learning rate 0.000247\n",
            "eval: split train. loss 1.175291e-01. error 3.51%. misses: 256\n",
            "eval: split test . loss 1.250654e-01. error 4.04%. misses: 81\n",
            "epoch 18 with learning rate 0.000243\n",
            "eval: split train. loss 1.103006e-01. error 3.36%. misses: 245\n",
            "eval: split test . loss 1.277010e-01. error 3.89%. misses: 78\n",
            "epoch 19 with learning rate 0.000240\n",
            "eval: split train. loss 1.236658e-01. error 3.66%. misses: 267\n",
            "eval: split test . loss 1.409360e-01. error 4.29%. misses: 86\n",
            "epoch 20 with learning rate 0.000237\n",
            "eval: split train. loss 1.020552e-01. error 3.13%. misses: 228\n",
            "eval: split test . loss 1.063714e-01. error 3.24%. misses: 65\n",
            "epoch 21 with learning rate 0.000233\n",
            "eval: split train. loss 1.091006e-01. error 3.55%. misses: 259\n",
            "eval: split test . loss 1.246423e-01. error 3.74%. misses: 75\n",
            "epoch 22 with learning rate 0.000230\n",
            "eval: split train. loss 1.106910e-01. error 3.46%. misses: 252\n",
            "eval: split test . loss 1.356218e-01. error 4.33%. misses: 87\n",
            "epoch 23 with learning rate 0.000227\n",
            "eval: split train. loss 9.659009e-02. error 2.89%. misses: 211\n",
            "eval: split test . loss 1.137179e-01. error 3.54%. misses: 71\n",
            "epoch 24 with learning rate 0.000223\n",
            "eval: split train. loss 9.269170e-02. error 2.88%. misses: 210\n",
            "eval: split test . loss 1.103029e-01. error 3.29%. misses: 66\n",
            "epoch 25 with learning rate 0.000220\n",
            "eval: split train. loss 9.949408e-02. error 3.17%. misses: 231\n",
            "eval: split test . loss 1.186032e-01. error 3.39%. misses: 68\n",
            "epoch 26 with learning rate 0.000217\n",
            "eval: split train. loss 1.021277e-01. error 3.07%. misses: 224\n",
            "eval: split test . loss 1.310033e-01. error 4.24%. misses: 85\n",
            "epoch 27 with learning rate 0.000213\n",
            "eval: split train. loss 9.028766e-02. error 2.96%. misses: 216\n",
            "eval: split test . loss 1.154710e-01. error 3.54%. misses: 71\n",
            "epoch 28 with learning rate 0.000210\n",
            "eval: split train. loss 8.330627e-02. error 2.65%. misses: 193\n",
            "eval: split test . loss 1.085655e-01. error 2.79%. misses: 56\n",
            "epoch 29 with learning rate 0.000207\n",
            "eval: split train. loss 8.432113e-02. error 2.61%. misses: 190\n",
            "eval: split test . loss 1.129539e-01. error 3.14%. misses: 63\n",
            "epoch 30 with learning rate 0.000203\n",
            "eval: split train. loss 9.552831e-02. error 2.91%. misses: 212\n",
            "eval: split test . loss 1.301139e-01. error 3.84%. misses: 77\n",
            "epoch 31 with learning rate 0.000200\n",
            "eval: split train. loss 8.422315e-02. error 2.55%. misses: 186\n",
            "eval: split test . loss 1.214384e-01. error 3.84%. misses: 77\n",
            "epoch 32 with learning rate 0.000197\n",
            "eval: split train. loss 7.748855e-02. error 2.43%. misses: 177\n",
            "eval: split test . loss 1.129787e-01. error 3.29%. misses: 66\n",
            "epoch 33 with learning rate 0.000193\n",
            "eval: split train. loss 7.688072e-02. error 2.41%. misses: 176\n",
            "eval: split test . loss 1.043441e-01. error 3.04%. misses: 61\n",
            "epoch 34 with learning rate 0.000190\n",
            "eval: split train. loss 7.577808e-02. error 2.36%. misses: 172\n",
            "eval: split test . loss 1.039759e-01. error 2.89%. misses: 58\n",
            "epoch 35 with learning rate 0.000187\n",
            "eval: split train. loss 7.647464e-02. error 2.30%. misses: 168\n",
            "eval: split test . loss 1.100989e-01. error 3.19%. misses: 64\n",
            "epoch 36 with learning rate 0.000183\n",
            "eval: split train. loss 7.789679e-02. error 2.39%. misses: 174\n",
            "eval: split test . loss 1.084814e-01. error 3.09%. misses: 62\n",
            "epoch 37 with learning rate 0.000180\n",
            "eval: split train. loss 8.076993e-02. error 2.41%. misses: 176\n",
            "eval: split test . loss 1.200747e-01. error 3.49%. misses: 70\n",
            "epoch 38 with learning rate 0.000177\n",
            "eval: split train. loss 7.616804e-02. error 2.46%. misses: 179\n",
            "eval: split test . loss 1.116122e-01. error 3.24%. misses: 65\n",
            "epoch 39 with learning rate 0.000173\n",
            "eval: split train. loss 7.238816e-02. error 2.19%. misses: 160\n",
            "eval: split test . loss 1.074002e-01. error 3.19%. misses: 64\n",
            "epoch 40 with learning rate 0.000170\n",
            "eval: split train. loss 6.768630e-02. error 2.11%. misses: 154\n",
            "eval: split test . loss 1.001078e-01. error 2.79%. misses: 56\n",
            "epoch 41 with learning rate 0.000167\n",
            "eval: split train. loss 7.117675e-02. error 2.24%. misses: 163\n",
            "eval: split test . loss 1.095481e-01. error 3.14%. misses: 63\n",
            "epoch 42 with learning rate 0.000163\n",
            "eval: split train. loss 6.562562e-02. error 1.96%. misses: 143\n",
            "eval: split test . loss 9.571616e-02. error 3.04%. misses: 61\n",
            "epoch 43 with learning rate 0.000160\n",
            "eval: split train. loss 6.772164e-02. error 2.18%. misses: 159\n",
            "eval: split test . loss 1.027936e-01. error 2.94%. misses: 59\n",
            "epoch 44 with learning rate 0.000157\n",
            "eval: split train. loss 6.018578e-02. error 1.84%. misses: 133\n",
            "eval: split test . loss 8.969305e-02. error 2.54%. misses: 51\n",
            "epoch 45 with learning rate 0.000153\n",
            "eval: split train. loss 6.475975e-02. error 1.98%. misses: 144\n",
            "eval: split test . loss 9.830346e-02. error 2.79%. misses: 56\n",
            "epoch 46 with learning rate 0.000150\n",
            "eval: split train. loss 6.167312e-02. error 1.85%. misses: 135\n",
            "eval: split test . loss 1.052086e-01. error 3.04%. misses: 61\n",
            "epoch 47 with learning rate 0.000147\n",
            "eval: split train. loss 6.286284e-02. error 1.92%. misses: 140\n",
            "eval: split test . loss 9.550936e-02. error 2.34%. misses: 47\n",
            "epoch 48 with learning rate 0.000143\n",
            "eval: split train. loss 6.140174e-02. error 1.85%. misses: 135\n",
            "eval: split test . loss 9.500626e-02. error 2.79%. misses: 56\n",
            "epoch 49 with learning rate 0.000140\n",
            "eval: split train. loss 5.730263e-02. error 1.76%. misses: 128\n",
            "eval: split test . loss 8.655685e-02. error 2.44%. misses: 49\n",
            "epoch 50 with learning rate 0.000137\n",
            "eval: split train. loss 6.365073e-02. error 1.91%. misses: 139\n",
            "eval: split test . loss 9.558235e-02. error 2.94%. misses: 59\n",
            "epoch 51 with learning rate 0.000133\n",
            "eval: split train. loss 6.145608e-02. error 2.00%. misses: 146\n",
            "eval: split test . loss 9.673962e-02. error 2.99%. misses: 60\n",
            "epoch 52 with learning rate 0.000130\n",
            "eval: split train. loss 6.675736e-02. error 2.04%. misses: 149\n",
            "eval: split test . loss 1.128674e-01. error 3.34%. misses: 67\n",
            "epoch 53 with learning rate 0.000127\n",
            "eval: split train. loss 6.009426e-02. error 1.89%. misses: 138\n",
            "eval: split test . loss 1.022279e-01. error 2.94%. misses: 59\n",
            "epoch 54 with learning rate 0.000123\n",
            "eval: split train. loss 5.881622e-02. error 1.82%. misses: 133\n",
            "eval: split test . loss 1.017893e-01. error 2.89%. misses: 58\n",
            "epoch 55 with learning rate 0.000120\n",
            "eval: split train. loss 5.256474e-02. error 1.58%. misses: 114\n",
            "eval: split test . loss 8.889767e-02. error 2.64%. misses: 53\n",
            "epoch 56 with learning rate 0.000117\n",
            "eval: split train. loss 5.478412e-02. error 1.67%. misses: 122\n",
            "eval: split test . loss 9.094577e-02. error 2.74%. misses: 55\n",
            "epoch 57 with learning rate 0.000113\n",
            "eval: split train. loss 6.008846e-02. error 1.84%. misses: 133\n",
            "eval: split test . loss 1.056642e-01. error 3.04%. misses: 61\n",
            "epoch 58 with learning rate 0.000110\n",
            "eval: split train. loss 5.453264e-02. error 1.76%. misses: 128\n",
            "eval: split test . loss 1.086442e-01. error 2.79%. misses: 56\n",
            "epoch 59 with learning rate 0.000107\n",
            "eval: split train. loss 5.047651e-02. error 1.60%. misses: 117\n",
            "eval: split test . loss 9.799584e-02. error 2.69%. misses: 54\n",
            "epoch 60 with learning rate 0.000103\n",
            "eval: split train. loss 5.098452e-02. error 1.65%. misses: 120\n",
            "eval: split test . loss 9.166716e-02. error 2.59%. misses: 52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change 1: replace tanh on last layer with FC and use softmax. Lower learning rate to 0.01\n",
        "\n",
        "```\n",
        "epoch 23\n",
        "eval: split train. loss 7.162272e-03. error 0.05%. misses: 4\n",
        "eval: split test . loss 1.687743e-01. error 4.14%. misses: 83\n",
        "\n",
        "```\n",
        "\n",
        "Change 2: change from SGD to AdamW with LR 3e-4, double epochs to 46, decay LR to 1e-4 over the course of training.\n",
        "\n",
        "```\n",
        "epoch 46\n",
        "eval: split train. loss 1.890260e-03. error 0.04%. misses: 2\n",
        "eval: split test . loss 1.953933e-01. error 4.04%. misses: 81\n",
        "```\n",
        "\n",
        "Change 3: Introduce data augmentation, e.g. a shift by at most 1 pixel in both x/y directions, and bump up training time to 60 epochs.\n",
        "\n",
        "```\n",
        "epoch 60\n",
        "eval: split train. loss 5.098452e-02. error 1.65%. misses: 120\n",
        "eval: split test . loss 9.166716e-02. error 2.59%. misses: 52\n",
        "```\n",
        "\n",
        "Change 4: add dropout at layer H3, shift activation function to relu, and bring up iterations to 80.\n",
        "\n",
        "```\n",
        "# 코드로 형식 지정됨\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8Ey5_RPUytFV"
      }
    }
  ]
}