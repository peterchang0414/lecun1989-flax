{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lecun_scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peterchang0414/lecun1989-flax/blob/main/lecun_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "oe2o-uZZd4yT"
      },
      "outputs": [],
      "source": [
        "!pip install -q flax"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "from flax import linen as nn\n",
        "from torchvision import datasets\n",
        "\n",
        "# Adapted from https://github.com/karpathy/lecun1989-repro/blob/master/prepro.py\n",
        "def get_datasets(n_tr, n_te):\n",
        "  train_test = {}\n",
        "  for split in {'train', 'test'}:\n",
        "    data = datasets.MNIST('./data', train=split=='train', download=True)\n",
        "    \n",
        "    n = n_tr if split == 'train' else n_te\n",
        "    key = jax.random.PRNGKey(0)\n",
        "    rp = jax.random.permutation(key, len(data))[:n]\n",
        "\n",
        "    X = jnp.full((n, 16, 16, 1), 0.0, dtype=jnp.float32)\n",
        "    Y = jnp.full((n, 10), -1.0, dtype=jnp.float32)\n",
        "    for i, ix in enumerate(rp):\n",
        "      I, yint = data[int(ix)]\n",
        "      xi = jnp.array(I, dtype=np.float32) / 127.5 - 1.0\n",
        "      xi = jax.image.resize(xi, (16, 16), 'bilinear')\n",
        "      X = X.at[i].set(np.expand_dims(xi, axis=2))\n",
        "      Y = Y.at[i, yint].set(1.0)\n",
        "    train_test[split] = (X, Y)\n",
        "  return train_test"
      ],
      "metadata": {
        "id": "pc7McMNRnt2E"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flax import linen as nn\n",
        "from flax.training import train_state\n",
        "from flax.linen.activation import tanh\n",
        "import optax\n",
        "from typing import Callable\n",
        "\n",
        "class Net(nn.Module):\n",
        "  bias_init: Callable = nn.initializers.zeros\n",
        "  kernel_init: Callable = nn.initializers.uniform()\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    # For weight initialization, Karpathy used numerator of 2.4 \n",
        "    # which is very close to sqrt(6) = 2.449... used by he_uniform()\n",
        "    # By default, weight-sharing forces bias-sharing and therefore\n",
        "    # we add the bias separately.\n",
        "    bias1 = self.param('bias1', self.bias_init, (8, 8, 12))\n",
        "    bias2 = self.param('bias2', self.bias_init, (4, 4, 12))\n",
        "    bias3 = self.param('bias3', self.bias_init, (30,))\n",
        "    bias4 = self.param('bias4', nn.initializers.constant(-1.0), (10,))\n",
        "    x = jnp.pad(x, [(0,0),(2,2),(2,2),(0,0)], constant_values=-1.0)\n",
        "    x = nn.Conv(features=12, kernel_size=(5,5), strides=2, padding='VALID',\n",
        "                use_bias=False, kernel_init=self.kernel_init)(x)\n",
        "    x = tanh(x + bias1)\n",
        "    x = jnp.pad(x, [(0,0),(2,2),(2,2),(0,0)], constant_values=-1.0)\n",
        "    x1, x2, x3 = x[..., 0:8], x[..., 4:12], jnp.concatenate((x[..., 0:4], x[..., 8:12]), axis=-1)\n",
        "    slice1 = nn.Conv(features=4, kernel_size=(5,5), strides=2, padding='VALID', \n",
        "                     use_bias=False, kernel_init=self.kernel_init)(x1)\n",
        "    slice2 = nn.Conv(features=4, kernel_size=(5,5), strides=2, padding='VALID',\n",
        "                     use_bias=False, kernel_init=self.kernel_init)(x2)\n",
        "    slice3 = nn.Conv(features=4, kernel_size=(5,5), strides=2, padding='VALID',\n",
        "                     use_bias=False, kernel_init=self.kernel_init)(x3)\n",
        "    # x = jnp.pad(x, [(0,0),(2,2),(2,2),(0,0)], constant_values=-1.0)\n",
        "    # x = nn.Conv(features=12, kernel_size=(5,5), strides=2, padding='VALID',\n",
        "    #             use_bias=False, kernel_init=nn.initializers.he_uniform())(x)\n",
        "    x = jnp.concatenate((slice1, slice2, slice3), axis=-1)\n",
        "    x = tanh(x + bias2)\n",
        "    x = x.reshape((x.shape[0], -1))\n",
        "    x = nn.Dense(features=30, use_bias=False)(x)\n",
        "    x = tanh(x + bias3)\n",
        "    x = nn.Dense(features=10, use_bias=False)(x)\n",
        "    x = tanh(x + bias4)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "hk3obOlvqoDZ"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def eval_step(params, X, Y):\n",
        "  Yhat = Net().apply({'params': params}, X)\n",
        "  loss = jnp.mean((Yhat - Y)**2)\n",
        "  err = jnp.mean(jnp.argmax(Y, -1) != jnp.argmax(Yhat, -1)).astype(float)\n",
        "  return loss, err"
      ],
      "metadata": {
        "id": "KUCi-jvDXuha"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_split(data, split, params):\n",
        "  X, Y = data[split]\n",
        "  loss, err = eval_step(params, X, Y)\n",
        "  print(f\"eval: split {split:5s}. loss {loss:e}. error {err*100:.2f}%. misses: {int(err*Y.shape[0])}\")"
      ],
      "metadata": {
        "id": "D9KDZtpgVUBu"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import value_and_grad\n",
        "import optax\n",
        "from flax.training import train_state\n",
        "\n",
        "def create_train_state(key, lr, X):\n",
        "  model = Net()\n",
        "  params = model.init(key, X)['params']\n",
        "  sgd_opt = optax.sgd(lr)\n",
        "  return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=sgd_opt)\n",
        "\n",
        "@jax.jit\n",
        "def train_step(state, X, Y):\n",
        "  def loss_fn(params):\n",
        "    Yhat = Net().apply({'params': params}, X)\n",
        "    loss = jnp.mean((Yhat - Y)**2)\n",
        "    err = jnp.mean(jnp.argmax(Y, -1) != jnp.argmax(Yhat, -1)).astype(float)\n",
        "    return loss, err\n",
        "  (_, Yhats), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
        "  state = state.apply_gradients(grads=grads)\n",
        "  return state\n",
        "\n",
        "def train_one_epoch(state, X, Y):\n",
        "  for step_num in range(X.shape[0]):\n",
        "    x, y = np.expand_dims(X[step_num], 0), np.expand_dims(Y[step_num], 0)\n",
        "    state = train_step(state, x, y)\n",
        "  return state\n",
        "\n",
        "def train(key, data, epochs, lr):\n",
        "  Xtr, Ytr = data['train']\n",
        "  Xte, Yte = data['test']\n",
        "  train_state = create_train_state(key, lr, Xtr)\n",
        "  for epoch in range(epochs):\n",
        "    print(f\"epoch {epoch+1}\")\n",
        "    train_state = train_one_epoch(train_state, Xtr, Ytr)\n",
        "    for split in ['train', 'test']:\n",
        "      eval_split(data, split, train_state.params)\n"
      ],
      "metadata": {
        "id": "nwzevZv3CdvA"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key = jax.random.PRNGKey(42)\n",
        "key, subkey = jax.random.split(key)\n",
        "\n",
        "train(key, get_datasets(7291, 2007), 23, 0.03)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZhSr9-YKlo5",
        "outputId": "aa2cabeb-7acf-4472-fdd6-0ad911863bc0"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1\n",
            "eval: split train. loss 6.434221e-02. error 8.97%. misses: 654\n",
            "eval: split test . loss 6.317881e-02. error 8.42%. misses: 169\n",
            "epoch 2\n",
            "eval: split train. loss 4.318366e-02. error 6.50%. misses: 474\n",
            "eval: split test . loss 4.527826e-02. error 6.53%. misses: 131\n",
            "epoch 3\n",
            "eval: split train. loss 3.489758e-02. error 5.09%. misses: 371\n",
            "eval: split test . loss 3.872835e-02. error 5.48%. misses: 110\n",
            "epoch 4\n",
            "eval: split train. loss 2.985972e-02. error 4.18%. misses: 305\n",
            "eval: split test . loss 3.550083e-02. error 5.13%. misses: 103\n",
            "epoch 5\n",
            "eval: split train. loss 2.571298e-02. error 3.55%. misses: 259\n",
            "eval: split test . loss 3.319864e-02. error 4.88%. misses: 98\n",
            "epoch 6\n",
            "eval: split train. loss 2.275874e-02. error 3.15%. misses: 229\n",
            "eval: split test . loss 3.197337e-02. error 4.63%. misses: 93\n",
            "epoch 7\n",
            "eval: split train. loss 2.049273e-02. error 2.85%. misses: 207\n",
            "eval: split test . loss 3.154708e-02. error 4.43%. misses: 89\n",
            "epoch 8\n",
            "eval: split train. loss 1.926711e-02. error 2.74%. misses: 199\n",
            "eval: split test . loss 3.223454e-02. error 4.48%. misses: 90\n",
            "epoch 9\n",
            "eval: split train. loss 1.848259e-02. error 2.62%. misses: 191\n",
            "eval: split test . loss 3.250705e-02. error 4.43%. misses: 89\n",
            "epoch 10\n",
            "eval: split train. loss 1.664246e-02. error 2.18%. misses: 159\n",
            "eval: split test . loss 3.191075e-02. error 4.53%. misses: 91\n",
            "epoch 11\n",
            "eval: split train. loss 1.478396e-02. error 1.92%. misses: 140\n",
            "eval: split test . loss 3.025987e-02. error 4.38%. misses: 88\n",
            "epoch 12\n",
            "eval: split train. loss 1.318055e-02. error 1.78%. misses: 129\n",
            "eval: split test . loss 2.943825e-02. error 4.24%. misses: 85\n",
            "epoch 13\n",
            "eval: split train. loss 1.253532e-02. error 1.70%. misses: 124\n",
            "eval: split test . loss 2.954069e-02. error 4.29%. misses: 86\n",
            "epoch 14\n",
            "eval: split train. loss 1.086872e-02. error 1.55%. misses: 113\n",
            "eval: split test . loss 2.840128e-02. error 4.14%. misses: 83\n",
            "epoch 15\n",
            "eval: split train. loss 1.052032e-02. error 1.47%. misses: 107\n",
            "eval: split test . loss 2.813762e-02. error 3.94%. misses: 79\n",
            "epoch 16\n",
            "eval: split train. loss 9.600867e-03. error 1.39%. misses: 101\n",
            "eval: split test . loss 2.841101e-02. error 3.89%. misses: 78\n",
            "epoch 17\n",
            "eval: split train. loss 9.670814e-03. error 1.36%. misses: 99\n",
            "eval: split test . loss 2.894965e-02. error 4.19%. misses: 84\n",
            "epoch 18\n",
            "eval: split train. loss 8.834886e-03. error 1.22%. misses: 89\n",
            "eval: split test . loss 2.886683e-02. error 4.14%. misses: 83\n",
            "epoch 19\n",
            "eval: split train. loss 8.145337e-03. error 1.11%. misses: 81\n",
            "eval: split test . loss 2.837077e-02. error 4.14%. misses: 83\n",
            "epoch 20\n",
            "eval: split train. loss 7.719864e-03. error 1.11%. misses: 81\n",
            "eval: split test . loss 2.835044e-02. error 4.38%. misses: 88\n",
            "epoch 21\n",
            "eval: split train. loss 7.340510e-03. error 1.01%. misses: 74\n",
            "eval: split test . loss 2.820185e-02. error 4.43%. misses: 89\n",
            "epoch 22\n",
            "eval: split train. loss 7.105745e-03. error 1.00%. misses: 73\n",
            "eval: split test . loss 2.839020e-02. error 4.38%. misses: 88\n",
            "epoch 23\n",
            "eval: split train. loss 6.788089e-03. error 0.97%. misses: 71\n",
            "eval: split test . loss 2.844171e-02. error 4.38%. misses: 88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "voQJfHf6thIH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}